LLM_PROVIDER = 'ollama'  # gemini, llama.cpp, or ollama
LLAMA_CPP_MODEL_PATH = '/home/kal/code/models/gemma-3-4b-it-UD-Q8_K_XL.gguf'
OLLAMA_MODEL = 'gemma3:latest'
OLLAMA_URL = 'http://localhost:11434'
GEMINI_MODEL = 'gemini-1.5-pro-latest'
DATABASE_URL = 'sqlite:///v-raptor.db'